---
title: "Practical Machine Learning - Week 4"
author: "Jessica Ginesta"
date: "March 31, 2019"
output: html_document

---

# Synopsis: 
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har

The goal of this project is to predict the manner in which the participants did the exercise.

From our results, we can say that the wight lifting exercises are being performed correctly.

## Data Processing

Let's start by downloading and extracting only the fields we will use for this analysis. We have followed a couple of steps to make our data cleaner.
-        We will remove poor predictors from our data by identifying which ones have zero variance.
-       We will remove the variables that have NA's.
-       We will remove the variables that are identifiers and timestamps and are not sensor measurements.

```{r global_options, include=FALSE,message=FALSE}
library(dplyr)
library(lubridate)
library(ggplot2)
library(caret)
library(ISLR)
library(rattle)
library(rpart)
library(randomForest)

knitr::opts_chunk$set(fig.width=15, fig.height=8,
                      echo=TRUE,eval=TRUE,options(scipen = 1, digits = 2))
```

```{r echo=TRUE,message=FALSE,results='hide',fig.keep='all',cache=TRUE}

#Load dataset
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header=TRUE)

testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header=TRUE)

#Cleaning data
# Remove variables that don't make intuitive sense for prediction which are the first 7.
trainClean<- training[, colSums(is.na(training)) == 0]
trainClean<-trainClean[,-c(1:7)]
testClean <- testing[, colSums(is.na(testing)) == 0]
testClean<-testClean[,-c(1:7)]

# Create a partition with the training dataset 
inTrain <- createDataPartition(trainClean$classe, p = 0.7, list = FALSE)
TrainSet <- trainClean[inTrain, ]
TestSet <- trainClean[-inTrain, ]

zeroVar <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -zeroVar]
TestSet  <- TestSet[,-zeroVar]
dim(TrainSet)
dim(TestSet)
```
Initially loading the data, the testing dataset had 20 observations with 160 variables and the training set had 19622 observations with 160 observations. Looking at the data there were many variables that had NA values in most of their observations. 
We also removed the first 7 columns as it is personal data and would interfere in the predictions.

After creatinga partition with the test and training set we have:
-       New Training Set: 13737 observations with 53 variables.
-       New Testing Set: 5885 observations with 53 variables.

### Analyzing Data
We will run our test and train data through a couple of models to obtain the best accuracy.
Cross validation is being used with k-folds=5, as 10 is more resource intesive and doesn't not improve accuracy that much.
We will start by setting the seed to allow a reproducible research.

```{r echo=TRUE,message=FALSE,fig.keep='all'}
set.seed(1111)

# Fit classification tess as a model
# <- trainControl(method="cv", number=5)
trControl <- trainControl(method="cv", number=5,allowParallel = TRUE)
modFit <- train(classe ~ .,method="rpart",data=na.omit(TrainSet),trControl=trControl)

# Print the classification tree
fancyRpartPlot(modFit$finalModel)
predTree <- predict(modFit,newdata=TestSet)
tableTree <- confusionMatrix(TestSet$classe,predTree)
tableTree
accuracyTree<-tableTree$overall[1]
accuracyTree
```
The accuracy of the Tree is `r accuracyTree` which is not very good. Our next step will be trying the random forest.

### Random Forest

We will now fit the model on modelRandomF, and instruct the "train" function to use 3-fold cross-validation to select optimal tuning parameters for the model.

```{r echo=TRUE,message=FALSE,fig.keep='all'}
rfControl <- trainControl(method="cv", number=3,verboseIter=F,allowParallel = TRUE)

modelRandomF <- train(classe~., data=na.omit(TrainSet), method="rf", trControl=rfControl)
print(modelRandomF)

# Print the accuracy
predRandomF <- predict(modelRandomF,newdata=TestSet)
tableRandomF <- confusionMatrix(TestSet$classe,predRandomF)
tableRandomF
accuracyRandomF<-tableRandomF$overall[1]
accuracyRandomF

plot(modelRandomF)

```
The accuracy of the random Forest is `r accuracyRandomF` which is great. Our next step will be trying to predict with the random forest model the Test data. The sample error rate is 0.01.

## Conclusion

Based on the accuracies above, the random forest model is the best one. We will now predict the values to use in the 20 questions test.

```{r echo=TRUE,message=FALSE,fig.keep='all',results='as-is' }

finalTest<-predict(modelRandomF,newdata=testClean)
finalTest

```

